{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST with Convolutional Neural Network\n",
    "\n",
    "To train regular neural network with images, we have been rescaling two dimensional image into one dimensional array. This means, we are losing a lot of information about relations between the pixels, which might lead to worse performance. Convolutional Neural networks are here to counter this issue down by keeping the relations between two (or more) dimensions of the input data, all while increasing the performance.\n",
    "\n",
    "MLP\n",
    "    - only use fully connected layers\n",
    "    - only accept vectors as input\n",
    "    \n",
    "CNN\n",
    "    - also use sparsely connected layers\n",
    "\n",
    "MLPs | CNNs\n",
    "--- | ---\n",
    "<img src=\"./neural_net2.jpeg\" width=\"400\" height=\"200\" /> | <img src=\"./cnn.jpeg\" width=\"400\" height=\"200\" />\n",
    "<center>Image source: http://cs231n.github.io/convolutional-networks/</center>\n",
    "\n",
    "In CNN, we can use `parameter sharing` between neurons to dramaticaly reduce number of parameters in the network. Parameter sharing can be used, because it's clear that if something is working for the pixel in the middle of an image, it will also work for another pixel of the image.\n",
    "\n",
    "MLP | CNN\n",
    "--- | ---\n",
    "<img src=\"./mnist-dense.png\" width=\"400\" height=\"200\" /> | <img src=\"./cnn-mnist.png\" width=\"400\" height=\"200\" />\n",
    "\n",
    "\n",
    "### Convolutional Layer\n",
    "Convolutional layer is set of filters, instead of clasic neurons, that are connected to the input layer. These filters then serve as neurons and have weights on their synapsis. Filters has their own width, height (in practice height and width are the same) and depth. Depth is equal to the number of channels in the image, so if we have a colored image, the depth is three (red, green, and blue).\n",
    "\n",
    "In convulutional layer we are working with <b>convolutional windows</b>. By sliding the window over the image we get new image with enhanced edges.\n",
    "\n",
    "![cnn_widnow.gif](./cnn_widnow.gif)\n",
    "<center>Image source: http://cs231n.github.io/convolutional-networks/</center>\n",
    "\n",
    "New image is calucalted by multiplying its matrix value with corresponding filter and then summing it up.\n",
    "\n",
    "<img src=\"./applying-filter.png\" width=\"500\" height=\"300\" />\n",
    "\n",
    "$$\n",
    " RELU\\Bigg(SUM\\Bigg(\\begin{matrix}\n",
    "  0*0 & 1*1 & 0*0 \\\\\n",
    "  0*0 & 1*1 & 0*0 \\\\\n",
    "  0*0 & 1*1 & 0*0\n",
    " \\end{matrix}\\Bigg)\\Bigg) = 3 \n",
    "$$\n",
    "\n",
    "Various filters are used to enhance various edges.\n",
    "\n",
    "<img src=\"./filters.png\" width=\"450\" height=\"250\" />\n",
    "\n",
    "By applying a filter, we are reducing the size of the image (height and width), but 'cause we are using multiple filters for each layer, we are significantly increasing its depth.\n",
    "\n",
    "<img src=\"./increasgin-depth.png\" width=\"400\" height=\"200\" />\n",
    "<center>Image source: https://keunwoochoi.wordpress.com/author/keunwoochoi/page/5/</center>\n",
    "    \n",
    "Sample of apyling some filters on the image and visualizing the edges:\n",
    "<img src=\"./car-filters.png\" width=\"600\" height=\"400\" />\n",
    "<center>Image source: https://github.com/udacity/aind2-cnn/blob/master/conv-visualization/conv_visualization.ipynb</center>\n",
    "  \n",
    "Convolutional widnows have multiple parameters, that can dramaticaly change the behviour of the network:\n",
    "\n",
    "- `Widnow size` is the width and height of the sliding window\n",
    "- `Strides` offset in pixels to move the window (most commonly set to 1 or 2)\n",
    "- `Padding` in case the `Strides` is greather than one, we need to decide what to do with pixels around the edges. We have two options:\n",
    "    - get rid of them: `valid`\n",
    "    - fill with zeros: `same`\n",
    "\n",
    "### Max Pooling\n",
    "\n",
    "Pooling helps us futher decrease number of parameters and prevent overfitting by downsampling the next representation of an image. It's common to pooling layer after each convolutional layer.\n",
    "There are multiple poolings variant:\n",
    "- `max pooling` is using window to chose a max value for the new cell of a matrix\n",
    "- `average pooling` averages the values of the widnow\n",
    "- `global average pooling` averages the value over whole layer\n",
    "\n",
    "<img src=\"./maxpool.jpeg\" width=\"400\" height=\"200\" />\n",
    "<center>Image source: http://cs231n.github.io/convolutional-networks/</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "print(\"The MNIST database has a training set of %d examples.\" % len(train_images))\n",
    "print(\"The MNIST database has a test set of %d examples.\" % len(test_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rescale\n",
    "Aditionaly to the rescale from <0,255> is convient for CNN to add chanel. We are using only one color for the training so the chanel is 1. In real world scenarios we will have 3 channels (RGB).\n",
    "MNIST dataset doesn't come with the chanel so we need to reshape it with one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def rescale_input(images):\n",
    "    scaled_images = images.astype('float32') / 255\n",
    "\n",
    "    # TODO write your rescle function in here\n",
    "    # rescale array from (x, 28, 28) to (x, 28, 28, 1) where x is the number of samples (images)\n",
    "    \n",
    "    return None\n",
    "\n",
    "## DO NOT MODIFY ANYTHING IN THIS CELL BELOW THIS LINE ##\n",
    "def test_cnn_input_shape(scale_function):\n",
    "    zeros = np.zeros((1000, 28, 28))\n",
    "    assert np.array_equal(scale_function(zeros).shape[1:], [28, 28, 1])\n",
    "    print('Test OK')\n",
    "    \n",
    "test_cnn_input_shape(rescale_input)\n",
    "scaled_train_images = rescale_input(train_images)\n",
    "scaled_test_images = rescale_input(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "print('Integer-valued labels:')\n",
    "print(train_labels[:10])\n",
    "\n",
    "# one-hot encode the labels\n",
    "train_labels = np_utils.to_categorical(train_labels, 10)\n",
    "test_labels = np_utils.to_categorical(test_labels, 10)\n",
    "\n",
    "# print first ten (one-hot) training labels\n",
    "print('One-hot labels:')\n",
    "print(test_labels[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model\n",
    "\n",
    "We are using Adam as an optimizer, cause Adam works really great with convolutional neural networks.\n",
    "\n",
    "Adam keeps learning rate for each synapsis and gradualy chaning it depended on the loss size.\n",
    "\n",
    "Try to define model so it will look something like this\n",
    "<img src=\"./sample_model.png\" width=\"500\" height=\"300\" />\n",
    "\n",
    "Also diffrent architectures will work, so don't be afraid if yours fully don't match mine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "\n",
    "# define the model\n",
    "K.clear_session()\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=16, kernel_size=2, padding='same', activation='relu',\n",
    "                 input_shape=(28, 28, 1)))  # 28 x 28 are images from MNIST plus 1 chanel (grey)\n",
    "\n",
    "# TODO add some cnn layer in here\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "# TODO maybe some dense layers here\n",
    "\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# summarize the model\n",
    "model.summary()\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# hyperparametters\n",
    "# TODO set number of epochs for the training\n",
    "epochs = 0\n",
    "\n",
    "# train the model\n",
    "checkpointer = ModelCheckpoint(filepath='mnist.model.best.hdf5',\n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "hist = model.fit(scaled_train_images, train_labels, batch_size=128, epochs=epochs,\n",
    "                 validation_split=0.2, callbacks=[checkpointer],\n",
    "                 verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('mnist.model.best.hdf5')\n",
    "\n",
    "score = model.evaluate(scaled_test_images, test_labels, verbose=0)\n",
    "accuracy = 100 * score[1]\n",
    "\n",
    "# print test accuracy\n",
    "print('Test accuracy: %.4f%%' % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "start = 15\n",
    "end = 25\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "\n",
    "for i in range(end-start):\n",
    "    prediction = int(model.predict(np.expand_dims(scaled_test_images[i], axis=0)).argmax())\n",
    "    display_image = test_images[i]\n",
    "    ax = fig.add_subplot(3, 12, i+1, xticks=[], yticks=[])\n",
    "    ax.imshow(display_image, cmap='gray')\n",
    "    ax.set_title(str(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance of visualization\n",
    "\n",
    "When working with something abstact, like neural networks where we are working with thousands of samples, we really need to keep on visualizing the process which helps us to significantly improve architecture and find potentional issues in our code. \n",
    "\n",
    "Here is a sample of visualizing what is important for the neurons while looking at the pixels, when they are trying to decide a class.\n",
    "\n",
    "<img src=\"./softmax-weights.png\" width=\"600\" height=\"400\" />\n",
    "<center>Image source: https://www.tensorflow.org/versions/r1.0/get_started/mnist/beginners</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your tasks\n",
    "\n",
    "Now here are tasks that you should do to get a little more familiar with this example:\n",
    "1. Change the parameters of the neural network - sizes of the convolutional layers etc.\n",
    "2. Add another convolutional layer or uncomment the commented code in the neural network\n",
    "3. Try to \"cripple\" the network as much as you can, to have for example accuracy around 50%. E.g. try increasing the dropout layers percentage to see what they are doing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
